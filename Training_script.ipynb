{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import os\n",
    "import shutil\n",
    "import pathlib as pl\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "#CUSTOM IMPORTS\n",
    "import Data_structures as DS\n",
    "import Train_utils as TU\n",
    "import SOAP_predictor as SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Datasets\n",
    "\n",
    "train_data = TU.MG_dataset(\"train\")\n",
    "val_data = TU.MG_dataset(\"val\")\n",
    "\n",
    "len_train = train_data.__len__()\n",
    "len_val = val_data.__len__()\n",
    "ratio = int(len_train/len_val) #must be integer multiples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "\n",
    "soap_pred = SP.SOAP_predictor_model().to(my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimiser\n",
    "optimiser = optim.Adam(soap_pred.parameters(), lr = 10**(-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "#minibatch size, same ratio in order to have the same number of minibatches\n",
    "train_minibatch = 10\n",
    "val_minibatch = int(train_minibatch/ratio)\n",
    "\n",
    "#used to store historical values for each epoch\n",
    "hist_training_loss = []\n",
    "hist_validation_loss = []\n",
    "\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    #create randomised access\n",
    "    train_batches = TU.custom_dataloader(len_train, train_minibatch).create_batches()\n",
    "    val_batches = TU.custom_dataloader(len_val, val_minibatch).create_batches()\n",
    "    \n",
    "    #initial loss over epoch - for historical record\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    \n",
    "    #batch counter\n",
    "    B_count = 0\n",
    "    V_count = 0\n",
    "    \n",
    "    for B, V in zip(train_batches, val_batches):\n",
    "        \n",
    "        train_loss = torch.tensor(0, dtype=torch.float, device = my_device)\n",
    "        val_loss = torch.tensor(0, dtype=torch.float, device = my_device)\n",
    "        \n",
    "        print(\"Epoch : \" + str(e))\n",
    "        \n",
    "        #TRAINING\n",
    "        print(\"Training batch : \" + str(B_count))\n",
    "        sample_count = 0\n",
    "        for b in B: #for sample in minibatch\n",
    "            \n",
    "            st = time.time()\n",
    "            with torch.no_grad():\n",
    "                #create lables and iter N-1 inputs\n",
    "                input_molecule = train_data.__getitem__(b)\n",
    "                #initialise MG object with labels\n",
    "                input_graph = DS.MoleculeGraph(input_molecule.atoms_list, input_molecule.SOAPs_list)\n",
    "                #assign iter N-1 inputs\n",
    "                input_graph.atoms = input_molecule.atoms_list_N_1\n",
    "                input_graph.species = input_molecule.species\n",
    "                input_graph.SOAPs = input_molecule.SOAPs_N_1\n",
    "                input_graph.nodes_by_class = input_molecule.nodes_by_class\n",
    "                input_graph.init_node_enc = input_molecule.init_node_enc\n",
    "            et = time.time()\n",
    "            \n",
    "            print(str(sample_count) + \" Molecule size: \" + str(len(input_graph.atom_labels)))\n",
    "            print(\"Elapsed time: \" + str(et-st))\n",
    "            \n",
    "            st = time.time()\n",
    "            #GO THROUGH LAST ITERATION STEP\n",
    "            output_graph_N = soap_pred(input_graph, net_state=1)\n",
    "            \n",
    "            #GO THROUGH FINALISATION\n",
    "            output_graph = soap_pred(output_graph_N, net_state=2)\n",
    "            et = time.time()\n",
    "            print(\"Elapsed time: \" + str(et-st))\n",
    "            \n",
    "            st = time.time()\n",
    "            #COMPUTE SAMPLE LOSS\n",
    "            tr_ls = TU.SOAP_loss(output_graph, output_graph_N)\n",
    "            et = time.time()\n",
    "            print(\"Elapsed time: \" + str(et-st))\n",
    "\n",
    "            print(\"Sample loss : \" + \"{:.4f}\".format(tr_ls.item()))\n",
    "            \n",
    "            #ACCUMUlATE LOSS\n",
    "            train_loss += tr_ls\n",
    "            \n",
    "            sample_count += 1\n",
    "        \n",
    "        #MEAN LOSS OVER BATCH\n",
    "        train_loss = train_loss/train_minibatch\n",
    "        \n",
    "        B_count += 1\n",
    "        \n",
    "        print(\"Training loss over batch: \" + \"{:.4f}\".format(train_loss.item()))\n",
    "        \n",
    "        #MODEL OPTIMISATION\n",
    "        print(\"Start BP\")\n",
    "        train_loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "        print(\"Finish BP\")\n",
    "        \n",
    "        #MODEL CHECKPOINTING\n",
    "        save_model_name = \"soap_pred_weights\"+ str(B_count) + \".pth\"\n",
    "        torch.save(soap_pred.state_dict(), save_model_name)\n",
    "        \n",
    "        #VALIDATION\n",
    "        print(\"Validation batch : \" + str(V_count))\n",
    "        with torch.no_grad():\n",
    "            sample_count = 0\n",
    "            for v in V:\n",
    "                \n",
    "                #create lables and iter N-1 inputs\n",
    "                input_molecule = train_data.__getitem__(b)\n",
    "                #initialise MG object with labels\n",
    "                input_graph = DS.MoleculeGraph(input_molecule.atoms_list, input_molecule.SOAPs_list)\n",
    "                #assign iter N-1 inputs\n",
    "                input_graph.atoms = input_molecule.atoms_list_N_1\n",
    "                input_graph.species = input_molecule.species\n",
    "                input_graph.SOAPs = input_molecule.SOAPs_N_1\n",
    "                input_graph.nodes_by_class = input_molecule.nodes_by_class\n",
    "                input_graph.init_node_enc = input_molecule.init_node_encan\n",
    "                \n",
    "                print(str(sample_count) + \" Molecule size: \" + str(len(input_graph.atom_labels)))\n",
    "                \n",
    "                #GO THROUGH LAST ITERATION STEP\n",
    "                output_graph_N = soap_pred(input_graph, net_state=1)\n",
    "            \n",
    "                #GO THROUGH FINALISATION\n",
    "                output_graph = soap_pred(output_graph_N, net_state=2)\n",
    "                \n",
    "                #COMPUTE SAMPLE LOSS\n",
    "                val_ls = TU.SOAP_loss(output_graph, output_graph_N)\n",
    "                \n",
    "                print(\"Sample loss : \" + \"{:.4f}\".format(tr_ls.item()))\n",
    "                \n",
    "                #ACCUMULATE LOSS\n",
    "                val_loss += val_ls\n",
    "                \n",
    "                sample_count += 1\n",
    "            \n",
    "            #MEAN LOSS OVER BATCH\n",
    "            val_loss = val_loss/val_minibatch\n",
    "            \n",
    "            V_count += 1\n",
    "            \n",
    "            print(\"Validation loss over batch: \" + \"{:.4f}\".format(val_loss.item()))\n",
    "            \n",
    "            #accumulate epoch values\n",
    "            training_loss += train_loss.item()\n",
    "            validation_loss += val_loss.item()\n",
    "            \n",
    "        #SAVE EPOCH VALUES\n",
    "        training_loss = training_loss/(len_train/train_minibatch)\n",
    "        validation_loss = validation_loss/(len_val/val_minibatch)\n",
    "\n",
    "        hist_training_loss.append([e, training_loss])\n",
    "        hist_validation_loss.append([e, validation_loss])\n",
    "        \n",
    "        np.savetxt(\"training_error.txt\", hist_training_loss)\n",
    "        np.savetxt(\"validation_error.txt\", hist_validation_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-afm",
   "language": "python",
   "name": "graph-afm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
